{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1cac091",
   "metadata": {},
   "source": [
    "## Logistic\tRegression\tvs.\tBayesian Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0e7776",
   "metadata": {},
   "source": [
    "Load Task1E_train.csv and Task1E_test.csv as well as the Bayesian classifier(BC) and logistic regression (LR) codes\tfrom\tActivities\t2 and 3 in Module\t3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b458a7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'mvtnorm' was built under R version 3.6.3\"Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n"
     ]
    }
   ],
   "source": [
    "library(mvtnorm) # generates multivariate Gaussian sampels and calculate the densities\n",
    "library(ggplot2) #Visualisation of data\n",
    "library(reshape2) #reshape the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da30d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the dataset for the task\n",
    "train <- read.csv(\"Task1E_train.csv\")\n",
    "test <- read.csv(\"Task1E_test.csv\")\n",
    "\n",
    "#Split the file\n",
    "train_data <- train[,-3]\n",
    "train_label <- train[,3]\n",
    "test_data <- test[,-3]\n",
    "test_label <- test[,3]\n",
    "\n",
    "#Label each can use either train/test_label since they got the same classes\n",
    "c1<-unique(train_label)[2] #Class1 or c0\n",
    "c2 <- unique(train_label)[1] #Class2 or c1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ab177",
   "metadata": {},
   "source": [
    "### Bayesian Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e71c9",
   "metadata": {},
   "source": [
    "These are the steps to build a bayesian Classifier (derieved from jupyter notebook:activity 3.2):\n",
    "<ol>\n",
    "\t<li>Calculate the class priors $p(\\mathcal{C}_k)$ based on the relative number of training data in each class,</li>\n",
    "\t<li>Calculate the class means $\\mu_k$, class covariance matrices $\\mathbf{S}_k$ and shared covariance matrix $\\Sigma$ using the training data,</li>\n",
    "\t<li>Using the estimated PDF function, calculate $p(x_n|\\mathcal{C}_k)$ for each data point and each class,</li>\n",
    "\t<li>For each test sample, find the class label $\\mathcal{C}_k$ that maximizes the $p(\\mathcal{C}_k)p(x_n|\\mathcal{C}_k)$,</li>\n",
    "</ol>\n",
    "\n",
    "In the following we take these steps one by one as the following below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3fc4e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_classifier <- function(train_set,class1,class2,train_lab,test_set,test_lab){\n",
    "    \n",
    "    #Calculate probability of the class in train dataset\n",
    "    pc1.hat <- sum(train_lab==class1)/nrow(train_set) #probability of getting c1\n",
    "    pc2.hat <- sum(train_lab==class2)/nrow(train_set) #probability of getting c2\n",
    "    \n",
    "    #Calculate class mean\n",
    "    muc1.hat <- colMeans(train_set[train_lab==class1,])\n",
    "    muc2.hat <- colMeans(train_set[train_lab==class2,])\n",
    "    \n",
    "    #Calculate class variance and shared covariance matrix\n",
    "    sigma_c1_hat <- var(train_set[train_lab==class1,]) #variance of class1\n",
    "    sigma_c2_hat <- var(train_set[train_lab==class2,]) #variance of class2\n",
    "    \n",
    "    sigma.hat <- pc1.hat * sigma_c1_hat + pc2.hat * sigma_c2_hat #Shared covariance matrix\n",
    "    \n",
    "    #Calculate the posterior probability based on the calculation above\n",
    "    posterior_c1 <- pc1.hat*dmvnorm(x=train_set, mean=muc1.hat, sigma=sigma.hat)\n",
    "    posterior_c2 <- pc2.hat*dmvnorm(x=train_set, mean=muc2.hat, sigma=sigma.hat)\n",
    "    \n",
    "    # calculate predictions:\n",
    "    train.predict <- ifelse(posterior_c1 > posterior_c2, class1, class2)\n",
    "    test.predict <- ifelse(pc1.hat*dmvnorm(x=test_set, mean=muc1.hat, sigma=sigma.hat) > \n",
    "                           pc2.hat*dmvnorm(x=test_set, mean=muc2.hat, sigma=sigma.hat), class1, class2)\n",
    "    \n",
    "    #Error calculation\n",
    "    train_error <- 1 - sum(train_lab==train.predict)/nrow(train_set)\n",
    "    test_error <- 1 - sum(test_lab==test.predict)/nrow(test_set)\n",
    "    \n",
    "    return(c(train_error,test_error))\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5e5a33",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc5189",
   "metadata": {},
   "source": [
    "Taking the following steps is neccesseary to build a logistic regression (derieved from jupyter notebook:activity 3.3):\n",
    "<ol>\n",
    "\t<li>Implement sigmoid function $\\sigma(\\pmb{w}.\\mathbf{x})$, and initialize weight vector $\\pmb{w}$, learning rate $\\eta$ and stopping criterion $\\epsilon$.</li>\n",
    "\t<li>Repeat the followings until the improvement becomes negligible (i.e., $|\\mathcal{L}(\\pmb{w}^{(\\tau+1)})-\\mathcal{L}(\\pmb{w}^{(\\tau)})| \\lt \\epsilon$):\n",
    "<ol>\n",
    "\t<li>Shuffle the training data</li>\n",
    "\t<li>For each datapoint in the training data do:\n",
    "<ol>\n",
    "\t<li>$\\pmb{w}^{(\\tau+1)} := \\pmb{w}^{(\\tau)} - \\eta (\\sigma(\\pmb{w}.\\mathbf{x}) - t_n) \\pmb{x}_n$</li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "In the followings, we implement each of these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff03ca",
   "metadata": {},
   "source": [
    "#### Auxilary Function & Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c808ee6a",
   "metadata": {},
   "source": [
    "Similar to other regression model, the auxilary function for calculate the outcome and the error is created as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe62d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function (=p(C1|X))\n",
    "sigmoid <- function(w, x){\n",
    "    return(1.0/(1.0+exp(-w%*%t(cbind(1,x)))))    \n",
    "}\n",
    "\n",
    "\n",
    "# auxiliary function that predicts class labels\n",
    "predict <- function(w, X, c0, c1){\n",
    "    sig <- sigmoid(w, X)\n",
    "    return(ifelse(sig>0.5, c1,c0))\n",
    "}\n",
    "    \n",
    "# auxiliary function that calculate a cost function\n",
    "cost <- function (w, X, T, c0){\n",
    "    sig <- sigmoid(w, X)\n",
    "    return(sum(ifelse(T==c0, 1-sig, sig)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2156d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg <- function(train_set,train_lab,test_set,test_lab,c0,c1){\n",
    "    # Initializations\n",
    "    tau.max <- 1000 # maximum number of iterations\n",
    "    eta <- 0.01 # learning rate\n",
    "    epsilon <- 0.01 # a threshold on the cost (to terminate the process)\n",
    "    tau <- 1 # iteration counter\n",
    "    terminate <- FALSE\n",
    "    \n",
    "    ## Just a few name/type conversion to make the rest of the code easy to follow\n",
    "    train_mat <- as.matrix(train_set) # rename just for conviniance\n",
    "    train_l <- ifelse(train_lab==c0,0,1) # rename just for conviniance\n",
    "\n",
    "    \n",
    "    W <- matrix(,nrow=tau.max, ncol=(ncol(train_mat)+1)) # to be used to store the estimated coefficients\n",
    "    W[1,] <- runif(ncol(W)) # initial weight (any better idea?)\n",
    "    \n",
    "    # project data using the sigmoid function (just for convenient)\n",
    "    Y <- sigmoid(W[1,],train_mat)\n",
    "    \n",
    "    costs <- data.frame('tau'=1:tau.max)  # to be used for trace the cost in each iteration\n",
    "    costs[1, 'cost_train'] <- cost(W[1,],train_mat,train_l, 0)\n",
    "    \n",
    "    while(!terminate){\n",
    "        # check termination criteria:\n",
    "        terminate <- tau >= tau.max | cost(W[tau,],train_mat,train_l, 0)<=epsilon\n",
    "        \n",
    "        # shuffle data:\n",
    "        train_index <- sample(1:nrow(train_set), nrow(train_set), replace = FALSE)\n",
    "        train_mat <- train_mat[train_index,]\n",
    "        train_l <- train_l[train_index]\n",
    "        \n",
    "        # for each datapoint:\n",
    "        for (i in 1:nrow(train_set)){\n",
    "            # check termination criteria:\n",
    "            if (tau >= tau.max | cost(W[tau,],train_mat,train_l, 0) <=epsilon) {terminate<-TRUE;break}\n",
    "            \n",
    "            Y <- sigmoid(W[tau,],train_mat)\n",
    "            \n",
    "            # Update the weights\n",
    "            W[(tau+1),] <- W[tau,] - eta * (Y[i]-train_l[i]) * cbind(1, t(train_mat[i,]))\n",
    "            \n",
    "            # record the cost:\n",
    "            costs[(tau+1), 'cost_train'] <- cost(W[tau,],train_mat,train_lab, 0)\n",
    "            \n",
    "            # update the counter:\n",
    "            tau <- tau + 1\n",
    "            \n",
    "            # decrease learning rate:\n",
    "            eta = eta * 0.999\n",
    "        }\n",
    "    }\n",
    "    # Done!\n",
    "    costs <- costs[1:tau, ] # remove the NaN tail of the vector (in case of early stopping)\n",
    "    # the  final result is:\n",
    "    w <- W[tau,]\n",
    "    #Error calculation\n",
    "    train_pred <- predict(w,train_set,c0,c1)\n",
    "    test_pred <- predict(w,test_set,c0,c1)\n",
    "    train_error <- 1-sum(train_lab==train_pred)/nrow(train_set)\n",
    "    test_error <- 1 - sum(test_lab==test_pred)/nrow(test_set)\n",
    "    \n",
    "    return(c(train_error,test_error))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08263634",
   "metadata": {},
   "source": [
    "Using the\t first 5 data points from the training set, train a BC and\t a LR model, and compute their test errors. In a “for loop”, increase\tthe\tsize\tof training set\t(5 data points at a time), retrain the models and calculate their test\terrors until all training data points are used. In one figure, plot the\ttest\terrors\tfor\teach\tmodel\t(with\tdifferent\tcolors)\tversus\tthe\tsize\tof\tthe training\tset;\tinclude\tthe\tplot\tin\tyour\tJupyter\tNotebook\tfile\t\n",
    "for\tQuestion\t7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87cde688",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(80)\n",
    "j = 1 #counter for each row that contain vector of logistic or bayes error\n",
    "#Store the testing error as vector\n",
    "logi_test_error <- c()\n",
    "bayes_test_error <- c()\n",
    "\n",
    "\n",
    "for (i in seq(5,to=nrow(train_data),by=5)){#For each data point start from five to all of data point moving by 5\n",
    "    #Train logistic regression model\n",
    "    logi_error_cal <- logistic_reg(train_data[1:i,], train_label[1:i], test_data, test_label,c1,c2)\n",
    "    logi_test_error[j] <- logi_error_cal[2] #store error from testing set based on the result of model prediction\n",
    "    \n",
    "    #Train bayesian classifier model\n",
    "    bayes_error <- bayes_classifier(train_data[1:i,],c1,c2,train_label[1:i], test_data, test_label)\n",
    "    bayes_test_error[j] <- bayes_error[2] #store error from testing set based on the result of model prediction\n",
    "    \n",
    "    j=j+1 #Increment of counter until the loop is end\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e767ecb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEUAAAAAv8RNTU1oaGh8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\nEBLABBASwAQQEsAE/APhlvewnCDM/gAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create dataframe for the plot\n",
    "test_error <- data.frame('training_size'=seq(5,to=nrow(train_data),by=5),'Logistic'=logi_test_error,'bayes'=bayes_test_error)\n",
    "test_error.melt <- melt(test_error,id='training_size') #Change form of the dataframe\n",
    "\n",
    "#Plot the test error per training size\n",
    "ggplot(test_error.melt,aes(x = training_size,y = value,color = variable)) +  geom_line()+\n",
    "scale_color_discrete(guide = guide_legend(title = 'Classifier type')) + theme_minimal()+ggtitle(\"Testing error per training size\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863951c",
   "metadata": {},
   "source": [
    "a. What does happen for the LR and BC respectively when the\tnumber of training data\tpoints\tis\tincreased?\n",
    "\n",
    "Answer: The error in classification is decreased in a similar pattern for both LR and BC model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef538a",
   "metadata": {},
   "source": [
    "b. Which classifier is best suited when the training set is small, and which\tis\tbest\tsuited\twhen\tthe\ttraining\tset\tis\tbig?\n",
    "\n",
    "Answer:Based on the plot, it seems that LR is suitable for small training set. However, in general, it is difficult to tell which model is better when the data size is very small because they will both perform poorly in prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9ccd88",
   "metadata": {},
   "source": [
    "c. Justify your observations in previous questions (III.a &\t III.b)\t by providing\tsome\tspeculations\tand\tpossible\treasons.Hint: Think about model complexity and the fundamental concepts\tof\tmachine\tlearning\tcovered\tin\tModule\t1.\n",
    "\n",
    "Answer: The reason for the plot to illustrate the following pattern because increasing the sample size(training size) will decrease the error causing by an overfitting (model is too complex) due to an increase in parameter of classifier model (non-parametric) along with increasing of training size. Therefore, increasing training size wil reduce error rate (Chen,2022a). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bc6820",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "All the code,algorithm and answer of the question is derieved from the following sources:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993b782",
   "metadata": {},
   "source": [
    "- Chen, B. (2022a). $\\textit{Week 1.:Elements of Machine Learning}$ \\[PowerPoint slides]. https://lms.monash.edu/mod/resource/view.php?id=9894948 \n",
    "- Chen, B. (2022b). $\\textit{Week 6.: Linear Models for Classification(Part B)}$ \\[PowerPoint slides]. https://lms.monash.edu/mod/resource/view.php?id=9895008\n",
    "- Jupyter Notebooks:FIT5201 Machine Learning, (nd.). $\\textit{Activity 3.2 Bayesian Classifier}$. \n",
    "https://lms.monash.edu/mod/folder/view.php?id=10133948\n",
    "- Jupyter Notebooks:FIT5201 Machine Learning, (nd.). $\\textit{Activity 3.3 Logistic Regression}$. \n",
    "https://lms.monash.edu/mod/folder/view.php?id=10133948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9de94e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
