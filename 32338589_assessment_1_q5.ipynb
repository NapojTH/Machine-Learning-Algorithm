{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b348b16",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba6437",
   "metadata": {},
   "source": [
    "1. Given\tthe\tgradient\tdescent\talgorithms\tfor\tlinear\tregression\t(discussed\tin\tChapter\t2\tof\tModule\t2),\tderive\tweight update\tsteps\tof\tstochastic\tgradient descent\t (SGD)\t as\t well\t as\t batch\t gradient\t descent\t (BGD)\t for\t linear\tregression\t with\t L2\t regularisation\t norm.\t Show\t your\t work\t with\t enough\texplanation\tin\tyour\tPDF\treport;\tyou\tshould\tprovide\tthe\tsteps\tof\tSGD\tand\tBGD,\tseparately.\t\n",
    "\n",
    "Hint: Recall\tthat\tfor\tlinear\tregression\twe\tdefined\tthe\terror\tfunction\tE\tand\t\n",
    "set\tits\tderivation\tto\tzero.\tFor\tthis\tassignment,\tyou\tonly\tneed\tto\tadd\tan\tL2\t\n",
    "regularization\t term\t to\t the\t error\t function\t and\t set\t the\t derivative\t of\t both\t\n",
    "terms\t (error\t term\tplus\t the\tregularization\t term)\t to\tzero.\tThis\tquestion\tis\t\n",
    "similar\tto\tActivity\t1\tof\tModule\t2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a7f9fe",
   "metadata": {},
   "source": [
    "### Using all necessary library for reshape and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8b36e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n"
     ]
    }
   ],
   "source": [
    "library(ggplot2) #Visualising the data\n",
    "library(reshape2) #reshape the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d02943",
   "metadata": {},
   "source": [
    "2. Using\t R\t (with\t no\t use\t of\t special libraries),\t implement\t SGD\t and\t BGD\talgorithms\t that\t you\t derived\t in\t Step\t I.\t The\t implementation\t is\tstraightforward\t as\t you\t are\t allowed\t to\t use\t the\t code\t examples\t from\tActivity\t1\tin\tModule\t2.\n",
    "\n",
    "\n",
    "Here is how the SGD and Batch algorithm is implemented:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dabce30",
   "metadata": {},
   "source": [
    "### Objective function of Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c48f47f",
   "metadata": {},
   "source": [
    "In the begining, the predict function (coefficient calculate) and the objective function of ridge regression is created as the following:\n",
    "\n",
    "Note: \n",
    "- $\\phi$ * w is the calculate of coefficient using for prediction in loss/objective function\n",
    "- objective function of this task is $\\frac{1}{2}$ * $\\sum$(true - $\\phi$ * w)^2 + $\\frac{\\lambda}{2}$*w^2 \n",
    "- $\\frac{\\lambda}{2}$*w^2 is regularization term of the L2 or ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd0f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function to calculate labels based on the estimated coefficients\n",
    "predict_func <- function(Phi, w){\n",
    "    return(Phi%*%w)\n",
    "} \n",
    "\n",
    "#Training Objective function for ridge regression\n",
    "train_ob <- function (Phi,w,lambda,label){\n",
    "    #L2 object function\n",
    "    return(mean((predict_func(Phi,w)-label)**2)+0.5*(lambda*w%*%w))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d76b132",
   "metadata": {},
   "source": [
    "Then, the sgd and batch algorithm will be created into a function as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b015a49",
   "metadata": {},
   "source": [
    "### SGD \n",
    "\n",
    "This algorithm is proceed by the following process:\n",
    "1. Initialise the parameter randomly and assign an initial weight\n",
    "2. Set the criteria for running the loop (learning rate(eta), termination criteria(criteria*sample_size), and epsilon(threshold)\n",
    "3. Create a loop to look through all each data point within all the training set\n",
    " 3.1 inside the loop: \n",
    " - the data is shuffled\n",
    " - the randomly pick data point will be used for weight updated\n",
    " - the weight is update by this formula: initial weight-eta*gradient(loss function)\n",
    " - if the iteration is reach the termination criteria or the weight is update to the similar or equal to the threshold the loop\n",
    "   will stop.\n",
    " 3.2 Update weight will be stored to the list as the the loop iterate until it find the minimal error that met epsilon\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3823c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD algorithm (Haffari,2016) \n",
    "sgd_algo <- function(train_data,train_label,lambda,eta,epsilon,max_epoch){\n",
    "    #initialise the function\n",
    "    #Max iteration\n",
    "    max_tau <- max_epoch * dim(train_data)[1]#number of sample in dataset * max_epoch\n",
    "    \n",
    "    #Store coefficient\n",
    "    weight <- matrix(,nrow=max_tau, ncol=ncol(train_data)) \n",
    "    weight[1,] <- runif(ncol(train_data)) #Initialise the weight\n",
    "    \n",
    "    #Setting the initial\n",
    "    tau <- 1\n",
    "    error_value <- matrix(,nrow=max_tau,ncol=1)\n",
    "    error_value[tau,1]<-train_ob(train_data,weight[tau,],lambda,train_label)\n",
    "    \n",
    "    #Iterative loop of SGD by setting the while loop to run when it is not get the minimise of loss function\n",
    "    while(tau <= max_tau){\n",
    "        #Criteria: when meet the minimal value, the loop will be break \n",
    "        if(error_value[tau,1]<=epsilon){break}\n",
    "        \n",
    "        #Shuffle the data point once one iteration is complete\n",
    "        index <- sample(1:dim(train_data)[1],dim(train_data)[1],replace=FALSE)\n",
    "        \n",
    "        #Loop over each data point from the dataset\n",
    "        for(i in index){\n",
    "            #Increment of each tau\n",
    "            tau <- tau+1\n",
    "            if (tau > max_tau) {break}\n",
    "\n",
    "            # make the weight update\n",
    "            y_pred <- predict_func(train_data[i,], weight[tau-1,])\n",
    "            # update weight vector\n",
    "            #L2 regularisation(gradient)\n",
    "            gradient <- -(train_label[i]-y_pred) \n",
    "            weight[tau,] <- weight[tau-1,]-eta*gradient\n",
    "\n",
    "            # keep track of the objective funtion\n",
    "            error_value[tau,1] <- train_ob(train_data, weight[tau,],lambda,train_label)\n",
    "            \n",
    "        }\n",
    "        \n",
    "    }\n",
    "    # resulting values for the training objective function as well as the weights\n",
    "   return(error_value)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d5d59b",
   "metadata": {},
   "source": [
    "### Batch Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5119da39",
   "metadata": {},
   "source": [
    "The process of this algorithm is similar to the SGD the only difference is that the data is not shuffling in the loop, the loop will look through all the data.\n",
    "\n",
    "Here is how the algorithm is implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abbf0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch algorithm (Haffari,2016)\n",
    "batch_grad_algo <- function(train_data,train_label,lambda,eta,epsilon,max_epoch){\n",
    "    \n",
    "    #Store the coefficient\n",
    "    weight <- matrix(,nrow=(max_epoch+1), ncol=ncol(train_data))\n",
    "    weight[1,] <- runif(ncol(train_data))\n",
    "    \n",
    "    #Initialise/iteration setting\n",
    "    tau <- 1 # counter \n",
    "    error_value <-matrix(,nrow=(max_epoch+1), ncol=1)\n",
    "    error_value[tau,1] <- train_ob(train_data, weight[tau,], lambda,train_label)\n",
    "    \n",
    "    #Since we run all the algorithm on the dataset all at once we only require a for loop to go through the data point without \n",
    "    #shuffling\n",
    "    for (tau in 1:max_epoch){\n",
    "        # check termination criteria, if the rate is met the minimal point it will break\n",
    "        if (train_ob(train_data,weight[tau,],lambda,train_label)<=epsilon) {break}\n",
    "        # make prediction over the training set\n",
    "        y_pred <- train_data %*% weight[tau,]\n",
    "        \n",
    "        # update the weight vector\n",
    "        gradient <- -colMeans(matrix((train_label-y_pred),nrow=dim(train_data)[1],ncol=dim(train_data)[2]) * train_data)\n",
    "        weight[tau+1,] <- weight[tau,] - eta * gradient\n",
    "        \n",
    "        # keep track of the objective funtion\n",
    "        error_value[tau+1,1] <- train_ob(train_data, weight[tau+1,],lambda,train_label)\n",
    "        \n",
    "   } \n",
    "   # resulting values for the training objective function as well as the weights\n",
    "   return(error_value)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5188c1",
   "metadata": {},
   "source": [
    "## Import the file & train the sgd and batch on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8573f280",
   "metadata": {},
   "source": [
    "1. Load Task1C_train.csv and\tTask1C_test.csv sets.\n",
    "2. Set\tthe\ttermination\tcriterion\tas\tmaximum\tof\t18 weight\tupdates\tfor\t BGD,\twhich\tis\tequivalent\tto\t18 xN weight updates for\tSGD\t(where\tN\tis\tthe\tnumber\tof\ttraining\tdata).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02964819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and scale the data\n",
    "train<- read.csv(\"Task1C_train.csv\")\n",
    "test<- read.csv(\"Task1C_test.csv\")\n",
    "train_data <- scale(train[,-5])\n",
    "train_label <- scale(train[,5])\n",
    "test_data <- scale(test[,-5])\n",
    "test_label <- scale(test[,5])\n",
    "\n",
    "#Initialising parameter\n",
    "max_epoch <- 18 #Max iteration\n",
    "epsilon <- .001 #treshold criteria\n",
    "eta <- .01 #learning rate\n",
    "lambda <- 0 #lambda is set to zero due to a derivative of error and regularizer (since we find a minimal value)\n",
    "\n",
    "#Take a while to run\n",
    "options(warn=-1)\n",
    "set.seed(6)\n",
    "sgd_res <- sgd_algo(train_data, train_label, lambda, eta, epsilon, max_epoch)\n",
    "batch_res <- batch_grad_algo(train_data, train_label, lambda, eta, epsilon, max_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028676c",
   "metadata": {},
   "source": [
    "### Plot the training error for Batch and SGD\n",
    "\n",
    "Run\t your\t implementations\t of\t SGD\t and\t BGD\t while\t all\t parameter\tsettings\t (initial\tvalues,\tlearning\trate\tetc)\tare\texactly\t the\tsame\t for\tboth\talgorithms.\tDuring\trun,\trecord\ttraining\terror rate\tevery\ttime the\tweights\tget\tupdated.\tCreate\ta\tplot\tof\terror\trates\t(use\tdifferent\tcolors\tfor\tSGD\tand\tBGD),\twhere\tthe\tx-axis\tis\tthe\tnumber\tof\tvisited\tdata\t points\t and\t y-axis\t is\t the\t error\t rate.\t Save your\t plot\t in\t your\tJupyter\tNotebook\t file\t for\tQuestion\t5. Note\tthat\t for\tevery\tN\terrors\t\n",
    "for\tSGD\tin\tthe\tplot,\tyou\twill\tonly\thave\tone\terror\tfor\tBGD;\tthe\ttotal length\tof\tthe\tx-axis\twill\tbe\t18x\tN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6b41e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>tau</th><th scope=col>type</th><th scope=col>error</th><th scope=col>algo</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 1      </td><td>Train   </td><td>1.259515</td><td>SGD     </td></tr>\n",
       "\t<tr><td> 2      </td><td>Train   </td><td>1.241558</td><td>SGD     </td></tr>\n",
       "\t<tr><td> 3      </td><td>Train   </td><td>1.246511</td><td>SGD     </td></tr>\n",
       "\t<tr><td> 4      </td><td>Train   </td><td>1.287707</td><td>SGD     </td></tr>\n",
       "\t<tr><td> 5      </td><td>Train   </td><td>1.166218</td><td>SGD     </td></tr>\n",
       "\t<tr><td> 6      </td><td>Train   </td><td>1.158333</td><td>SGD     </td></tr>\n",
       "\t<tr><td> 7      </td><td>Train   </td><td>1.224018</td><td>SGD     </td></tr>\n",
       "\t<tr><td> 8      </td><td>Train   </td><td>1.327647</td><td>SGD     </td></tr>\n",
       "\t<tr><td> 9      </td><td>Train   </td><td>1.317515</td><td>SGD     </td></tr>\n",
       "\t<tr><td>10      </td><td>Train   </td><td>1.395023</td><td>SGD     </td></tr>\n",
       "\t<tr><td>11      </td><td>Train   </td><td>1.456228</td><td>SGD     </td></tr>\n",
       "\t<tr><td>12      </td><td>Train   </td><td>1.488909</td><td>SGD     </td></tr>\n",
       "\t<tr><td>13      </td><td>Train   </td><td>1.584653</td><td>SGD     </td></tr>\n",
       "\t<tr><td>14      </td><td>Train   </td><td>1.615804</td><td>SGD     </td></tr>\n",
       "\t<tr><td>15      </td><td>Train   </td><td>1.658000</td><td>SGD     </td></tr>\n",
       "\t<tr><td>16      </td><td>Train   </td><td>1.479963</td><td>SGD     </td></tr>\n",
       "\t<tr><td>17      </td><td>Train   </td><td>1.526257</td><td>SGD     </td></tr>\n",
       "\t<tr><td>18      </td><td>Train   </td><td>1.660844</td><td>SGD     </td></tr>\n",
       "\t<tr><td>19      </td><td>Train   </td><td>1.638805</td><td>SGD     </td></tr>\n",
       "\t<tr><td>20      </td><td>Train   </td><td>1.673746</td><td>SGD     </td></tr>\n",
       "\t<tr><td>21      </td><td>Train   </td><td>1.712152</td><td>SGD     </td></tr>\n",
       "\t<tr><td>22      </td><td>Train   </td><td>1.822523</td><td>SGD     </td></tr>\n",
       "\t<tr><td>23      </td><td>Train   </td><td>1.639616</td><td>SGD     </td></tr>\n",
       "\t<tr><td>24      </td><td>Train   </td><td>1.668856</td><td>SGD     </td></tr>\n",
       "\t<tr><td>25      </td><td>Train   </td><td>1.702929</td><td>SGD     </td></tr>\n",
       "\t<tr><td>26      </td><td>Train   </td><td>1.598645</td><td>SGD     </td></tr>\n",
       "\t<tr><td>27      </td><td>Train   </td><td>1.622762</td><td>SGD     </td></tr>\n",
       "\t<tr><td>28      </td><td>Train   </td><td>1.743038</td><td>SGD     </td></tr>\n",
       "\t<tr><td>29      </td><td>Train   </td><td>1.877908</td><td>SGD     </td></tr>\n",
       "\t<tr><td>30      </td><td>Train   </td><td>1.848474</td><td>SGD     </td></tr>\n",
       "\t<tr><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "\t<tr><td>16730     </td><td>Train     </td><td>0.02224280</td><td>SGD       </td></tr>\n",
       "\t<tr><td>16731     </td><td>Train     </td><td>0.02258661</td><td>SGD       </td></tr>\n",
       "\t<tr><td>16732     </td><td>Train     </td><td>0.02252557</td><td>SGD       </td></tr>\n",
       "\t<tr><td>16733     </td><td>Train     </td><td>0.02259128</td><td>SGD       </td></tr>\n",
       "\t<tr><td>16734     </td><td>Train     </td><td>0.02268673</td><td>SGD       </td></tr>\n",
       "\t<tr><td>16735     </td><td>Train     </td><td>0.02234656</td><td>SGD       </td></tr>\n",
       "\t<tr><td>16736     </td><td>Train     </td><td>0.02237212</td><td>SGD       </td></tr>\n",
       "\t<tr><td>16737     </td><td>Train     </td><td>0.02207637</td><td>SGD       </td></tr>\n",
       "\t<tr><td>16738     </td><td>Train     </td><td>0.02197760</td><td>SGD       </td></tr>\n",
       "\t<tr><td>16739     </td><td>Train     </td><td>0.02214478</td><td>SGD       </td></tr>\n",
       "\t<tr><td>16740     </td><td>Train     </td><td>0.02253026</td><td>SGD       </td></tr>\n",
       "\t<tr><td>  930     </td><td>Train     </td><td>0.12705910</td><td>batch     </td></tr>\n",
       "\t<tr><td> 1860     </td><td>Train     </td><td>0.11903727</td><td>batch     </td></tr>\n",
       "\t<tr><td> 2790     </td><td>Train     </td><td>0.11159671</td><td>batch     </td></tr>\n",
       "\t<tr><td> 3720     </td><td>Train     </td><td>0.10469529</td><td>batch     </td></tr>\n",
       "\t<tr><td> 4650     </td><td>Train     </td><td>0.09829393</td><td>batch     </td></tr>\n",
       "\t<tr><td> 5580     </td><td>Train     </td><td>0.09235641</td><td>batch     </td></tr>\n",
       "\t<tr><td> 6510     </td><td>Train     </td><td>0.08684909</td><td>batch     </td></tr>\n",
       "\t<tr><td> 7440     </td><td>Train     </td><td>0.08174080</td><td>batch     </td></tr>\n",
       "\t<tr><td> 8370     </td><td>Train     </td><td>0.07700264</td><td>batch     </td></tr>\n",
       "\t<tr><td> 9300     </td><td>Train     </td><td>0.07260776</td><td>batch     </td></tr>\n",
       "\t<tr><td>10230     </td><td>Train     </td><td>0.06853130</td><td>batch     </td></tr>\n",
       "\t<tr><td>11160     </td><td>Train     </td><td>0.06475018</td><td>batch     </td></tr>\n",
       "\t<tr><td>12090     </td><td>Train     </td><td>0.06124299</td><td>batch     </td></tr>\n",
       "\t<tr><td>13020     </td><td>Train     </td><td>0.05798988</td><td>batch     </td></tr>\n",
       "\t<tr><td>13950     </td><td>Train     </td><td>0.05497245</td><td>batch     </td></tr>\n",
       "\t<tr><td>14880     </td><td>Train     </td><td>0.05217360</td><td>batch     </td></tr>\n",
       "\t<tr><td>15810     </td><td>Train     </td><td>0.04957750</td><td>batch     </td></tr>\n",
       "\t<tr><td>16740     </td><td>Train     </td><td>0.04716945</td><td>batch     </td></tr>\n",
       "\t<tr><td>17670     </td><td>Train     </td><td>0.04493584</td><td>batch     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " tau & type & error & algo\\\\\n",
       "\\hline\n",
       "\t  1       & Train    & 1.259515 & SGD     \\\\\n",
       "\t  2       & Train    & 1.241558 & SGD     \\\\\n",
       "\t  3       & Train    & 1.246511 & SGD     \\\\\n",
       "\t  4       & Train    & 1.287707 & SGD     \\\\\n",
       "\t  5       & Train    & 1.166218 & SGD     \\\\\n",
       "\t  6       & Train    & 1.158333 & SGD     \\\\\n",
       "\t  7       & Train    & 1.224018 & SGD     \\\\\n",
       "\t  8       & Train    & 1.327647 & SGD     \\\\\n",
       "\t  9       & Train    & 1.317515 & SGD     \\\\\n",
       "\t 10       & Train    & 1.395023 & SGD     \\\\\n",
       "\t 11       & Train    & 1.456228 & SGD     \\\\\n",
       "\t 12       & Train    & 1.488909 & SGD     \\\\\n",
       "\t 13       & Train    & 1.584653 & SGD     \\\\\n",
       "\t 14       & Train    & 1.615804 & SGD     \\\\\n",
       "\t 15       & Train    & 1.658000 & SGD     \\\\\n",
       "\t 16       & Train    & 1.479963 & SGD     \\\\\n",
       "\t 17       & Train    & 1.526257 & SGD     \\\\\n",
       "\t 18       & Train    & 1.660844 & SGD     \\\\\n",
       "\t 19       & Train    & 1.638805 & SGD     \\\\\n",
       "\t 20       & Train    & 1.673746 & SGD     \\\\\n",
       "\t 21       & Train    & 1.712152 & SGD     \\\\\n",
       "\t 22       & Train    & 1.822523 & SGD     \\\\\n",
       "\t 23       & Train    & 1.639616 & SGD     \\\\\n",
       "\t 24       & Train    & 1.668856 & SGD     \\\\\n",
       "\t 25       & Train    & 1.702929 & SGD     \\\\\n",
       "\t 26       & Train    & 1.598645 & SGD     \\\\\n",
       "\t 27       & Train    & 1.622762 & SGD     \\\\\n",
       "\t 28       & Train    & 1.743038 & SGD     \\\\\n",
       "\t 29       & Train    & 1.877908 & SGD     \\\\\n",
       "\t 30       & Train    & 1.848474 & SGD     \\\\\n",
       "\t ... & ... & ... & ...\\\\\n",
       "\t 16730      & Train      & 0.02224280 & SGD       \\\\\n",
       "\t 16731      & Train      & 0.02258661 & SGD       \\\\\n",
       "\t 16732      & Train      & 0.02252557 & SGD       \\\\\n",
       "\t 16733      & Train      & 0.02259128 & SGD       \\\\\n",
       "\t 16734      & Train      & 0.02268673 & SGD       \\\\\n",
       "\t 16735      & Train      & 0.02234656 & SGD       \\\\\n",
       "\t 16736      & Train      & 0.02237212 & SGD       \\\\\n",
       "\t 16737      & Train      & 0.02207637 & SGD       \\\\\n",
       "\t 16738      & Train      & 0.02197760 & SGD       \\\\\n",
       "\t 16739      & Train      & 0.02214478 & SGD       \\\\\n",
       "\t 16740      & Train      & 0.02253026 & SGD       \\\\\n",
       "\t   930      & Train      & 0.12705910 & batch     \\\\\n",
       "\t  1860      & Train      & 0.11903727 & batch     \\\\\n",
       "\t  2790      & Train      & 0.11159671 & batch     \\\\\n",
       "\t  3720      & Train      & 0.10469529 & batch     \\\\\n",
       "\t  4650      & Train      & 0.09829393 & batch     \\\\\n",
       "\t  5580      & Train      & 0.09235641 & batch     \\\\\n",
       "\t  6510      & Train      & 0.08684909 & batch     \\\\\n",
       "\t  7440      & Train      & 0.08174080 & batch     \\\\\n",
       "\t  8370      & Train      & 0.07700264 & batch     \\\\\n",
       "\t  9300      & Train      & 0.07260776 & batch     \\\\\n",
       "\t 10230      & Train      & 0.06853130 & batch     \\\\\n",
       "\t 11160      & Train      & 0.06475018 & batch     \\\\\n",
       "\t 12090      & Train      & 0.06124299 & batch     \\\\\n",
       "\t 13020      & Train      & 0.05798988 & batch     \\\\\n",
       "\t 13950      & Train      & 0.05497245 & batch     \\\\\n",
       "\t 14880      & Train      & 0.05217360 & batch     \\\\\n",
       "\t 15810      & Train      & 0.04957750 & batch     \\\\\n",
       "\t 16740      & Train      & 0.04716945 & batch     \\\\\n",
       "\t 17670      & Train      & 0.04493584 & batch     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| tau | type | error | algo |\n",
       "|---|---|---|---|\n",
       "|  1       | Train    | 1.259515 | SGD      |\n",
       "|  2       | Train    | 1.241558 | SGD      |\n",
       "|  3       | Train    | 1.246511 | SGD      |\n",
       "|  4       | Train    | 1.287707 | SGD      |\n",
       "|  5       | Train    | 1.166218 | SGD      |\n",
       "|  6       | Train    | 1.158333 | SGD      |\n",
       "|  7       | Train    | 1.224018 | SGD      |\n",
       "|  8       | Train    | 1.327647 | SGD      |\n",
       "|  9       | Train    | 1.317515 | SGD      |\n",
       "| 10       | Train    | 1.395023 | SGD      |\n",
       "| 11       | Train    | 1.456228 | SGD      |\n",
       "| 12       | Train    | 1.488909 | SGD      |\n",
       "| 13       | Train    | 1.584653 | SGD      |\n",
       "| 14       | Train    | 1.615804 | SGD      |\n",
       "| 15       | Train    | 1.658000 | SGD      |\n",
       "| 16       | Train    | 1.479963 | SGD      |\n",
       "| 17       | Train    | 1.526257 | SGD      |\n",
       "| 18       | Train    | 1.660844 | SGD      |\n",
       "| 19       | Train    | 1.638805 | SGD      |\n",
       "| 20       | Train    | 1.673746 | SGD      |\n",
       "| 21       | Train    | 1.712152 | SGD      |\n",
       "| 22       | Train    | 1.822523 | SGD      |\n",
       "| 23       | Train    | 1.639616 | SGD      |\n",
       "| 24       | Train    | 1.668856 | SGD      |\n",
       "| 25       | Train    | 1.702929 | SGD      |\n",
       "| 26       | Train    | 1.598645 | SGD      |\n",
       "| 27       | Train    | 1.622762 | SGD      |\n",
       "| 28       | Train    | 1.743038 | SGD      |\n",
       "| 29       | Train    | 1.877908 | SGD      |\n",
       "| 30       | Train    | 1.848474 | SGD      |\n",
       "| ... | ... | ... | ... |\n",
       "| 16730      | Train      | 0.02224280 | SGD        |\n",
       "| 16731      | Train      | 0.02258661 | SGD        |\n",
       "| 16732      | Train      | 0.02252557 | SGD        |\n",
       "| 16733      | Train      | 0.02259128 | SGD        |\n",
       "| 16734      | Train      | 0.02268673 | SGD        |\n",
       "| 16735      | Train      | 0.02234656 | SGD        |\n",
       "| 16736      | Train      | 0.02237212 | SGD        |\n",
       "| 16737      | Train      | 0.02207637 | SGD        |\n",
       "| 16738      | Train      | 0.02197760 | SGD        |\n",
       "| 16739      | Train      | 0.02214478 | SGD        |\n",
       "| 16740      | Train      | 0.02253026 | SGD        |\n",
       "|   930      | Train      | 0.12705910 | batch      |\n",
       "|  1860      | Train      | 0.11903727 | batch      |\n",
       "|  2790      | Train      | 0.11159671 | batch      |\n",
       "|  3720      | Train      | 0.10469529 | batch      |\n",
       "|  4650      | Train      | 0.09829393 | batch      |\n",
       "|  5580      | Train      | 0.09235641 | batch      |\n",
       "|  6510      | Train      | 0.08684909 | batch      |\n",
       "|  7440      | Train      | 0.08174080 | batch      |\n",
       "|  8370      | Train      | 0.07700264 | batch      |\n",
       "|  9300      | Train      | 0.07260776 | batch      |\n",
       "| 10230      | Train      | 0.06853130 | batch      |\n",
       "| 11160      | Train      | 0.06475018 | batch      |\n",
       "| 12090      | Train      | 0.06124299 | batch      |\n",
       "| 13020      | Train      | 0.05798988 | batch      |\n",
       "| 13950      | Train      | 0.05497245 | batch      |\n",
       "| 14880      | Train      | 0.05217360 | batch      |\n",
       "| 15810      | Train      | 0.04957750 | batch      |\n",
       "| 16740      | Train      | 0.04716945 | batch      |\n",
       "| 17670      | Train      | 0.04493584 | batch      |\n",
       "\n"
      ],
      "text/plain": [
       "      tau   type  error      algo \n",
       "1      1    Train 1.259515   SGD  \n",
       "2      2    Train 1.241558   SGD  \n",
       "3      3    Train 1.246511   SGD  \n",
       "4      4    Train 1.287707   SGD  \n",
       "5      5    Train 1.166218   SGD  \n",
       "6      6    Train 1.158333   SGD  \n",
       "7      7    Train 1.224018   SGD  \n",
       "8      8    Train 1.327647   SGD  \n",
       "9      9    Train 1.317515   SGD  \n",
       "10    10    Train 1.395023   SGD  \n",
       "11    11    Train 1.456228   SGD  \n",
       "12    12    Train 1.488909   SGD  \n",
       "13    13    Train 1.584653   SGD  \n",
       "14    14    Train 1.615804   SGD  \n",
       "15    15    Train 1.658000   SGD  \n",
       "16    16    Train 1.479963   SGD  \n",
       "17    17    Train 1.526257   SGD  \n",
       "18    18    Train 1.660844   SGD  \n",
       "19    19    Train 1.638805   SGD  \n",
       "20    20    Train 1.673746   SGD  \n",
       "21    21    Train 1.712152   SGD  \n",
       "22    22    Train 1.822523   SGD  \n",
       "23    23    Train 1.639616   SGD  \n",
       "24    24    Train 1.668856   SGD  \n",
       "25    25    Train 1.702929   SGD  \n",
       "26    26    Train 1.598645   SGD  \n",
       "27    27    Train 1.622762   SGD  \n",
       "28    28    Train 1.743038   SGD  \n",
       "29    29    Train 1.877908   SGD  \n",
       "30    30    Train 1.848474   SGD  \n",
       "...   ...   ...   ...        ...  \n",
       "16730 16730 Train 0.02224280 SGD  \n",
       "16731 16731 Train 0.02258661 SGD  \n",
       "16732 16732 Train 0.02252557 SGD  \n",
       "16733 16733 Train 0.02259128 SGD  \n",
       "16734 16734 Train 0.02268673 SGD  \n",
       "16735 16735 Train 0.02234656 SGD  \n",
       "16736 16736 Train 0.02237212 SGD  \n",
       "16737 16737 Train 0.02207637 SGD  \n",
       "16738 16738 Train 0.02197760 SGD  \n",
       "16739 16739 Train 0.02214478 SGD  \n",
       "16740 16740 Train 0.02253026 SGD  \n",
       "16741   930 Train 0.12705910 batch\n",
       "16742  1860 Train 0.11903727 batch\n",
       "16743  2790 Train 0.11159671 batch\n",
       "16744  3720 Train 0.10469529 batch\n",
       "16745  4650 Train 0.09829393 batch\n",
       "16746  5580 Train 0.09235641 batch\n",
       "16747  6510 Train 0.08684909 batch\n",
       "16748  7440 Train 0.08174080 batch\n",
       "16749  8370 Train 0.07700264 batch\n",
       "16750  9300 Train 0.07260776 batch\n",
       "16751 10230 Train 0.06853130 batch\n",
       "16752 11160 Train 0.06475018 batch\n",
       "16753 12090 Train 0.06124299 batch\n",
       "16754 13020 Train 0.05798988 batch\n",
       "16755 13950 Train 0.05497245 batch\n",
       "16756 14880 Train 0.05217360 batch\n",
       "16757 15810 Train 0.04957750 batch\n",
       "16758 16740 Train 0.04716945 batch\n",
       "16759 17670 Train 0.04493584 batch"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Reshape the sgd and rename some variables for convenience in visualisation\n",
    "sgd.melt <- melt(sgd_res)\n",
    "names(sgd.melt) <- c('tau','type','error')\n",
    "sgd.melt[\"type\"][sgd.melt['type']=='1'] <- 'Train'#Change the 1 inside the type column to train \n",
    "sgd.melt[,\"algo\"] <- 'SGD' #create more column to classified the algorithm for the result after apply the sgd algorithm\n",
    "\n",
    "\n",
    "#Reshape the batch and rename some variables for convenience in visualisation\n",
    "batch.melt <- melt(batch_res)\n",
    "names(batch.melt) <- c('tau','type','error')\n",
    "batch.melt[\"type\"][batch.melt['type']=='1'] <- 'Train' #Change the 1 inside the type column to train\n",
    "batch.melt[,\"algo\"] <- 'batch' #create more column to classified the algorithm for the result after apply the batch algorithm\n",
    "batch.melt[,'tau']<-batch.melt[,'tau']*dim(train_data)[1] #So the multiplication of sample size to iteration is require based \n",
    "#on the information above 18*n  so the 1 point of sgd match the one point of batch\n",
    "\n",
    "#Combine two reshape dataframe of the result of sgd and batch\n",
    "sgd_batch <- rbind(sgd.melt,batch.melt)\n",
    "sgd_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a3e78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEUAAAAAv8RNTU1oaGh8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\nSUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot Comparing training error between SGD and Batch\n",
    "ggplot(data=sgd_batch, aes(x=tau, y=error, color=algo)) +\n",
    "    geom_line() + ggtitle('Train Error (SGD vs Batch)') +\n",
    "    scale_color_discrete(guide = guide_legend(title = 'Errors')) +theme_minimal()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f655afb",
   "metadata": {},
   "source": [
    "Explain\t(in\tyour\tJupyter\tNotebook\tfile)\tyour\tobservation\tbased\ton\tthe\t errors\t plot\t you\t generated\t in\t Part\t c.\t Particularly,\t discuss the convergence\t speed\t and\t the\t fluctuations\t you\t see\t in\t the\t error trends.\n",
    "\n",
    "Based on the plot result, it seems that the sgd convergence speed is much quicker than the batch convergence speed. The reason for the differences in the convergence might be how sgd visit each data point in each iteration but not the whole dataset, which make the convergence almost near zero faster than batch. However, visiting some of the point might cause a fluctuation in error trend, which might be due to a variation in data point and how complicate the weight vector in each iteration get updated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab532691",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531d2749",
   "metadata": {},
   "source": [
    "All of the code and the algorithm idea is derieved from:\n",
    "\n",
    "- Chen, B. (2022). $\\textit{Week 3.: Linear Models for Regression}$ \\[PowerPoint slides]. https://lms.monash.edu/mod/resource/view.php?id=9894969\n",
    "- Haffari, G. (2016, July). $\\textit{CodeBase_A1_Q5 (1).R}$. https://lms.monash.edu/pluginfile.php/14028235/mod_assign/intro/CodeBase_A1_Q5%20%281%29.R\n",
    "- Haffari, G. (2019, January 9th). $\\textit{ Linear Models for Regression}$.\n",
    "https://lms.monash.edu/mod/resource/view.php?id=10099576\n",
    "- Jupyter Notebooks:FIT5201 Machine Learning, (nd.). $\\textit{Activity 2.1 Linear Regression}$. https://lms.monash.edu/mod/folder/view.php?id=10099584"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124be2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
