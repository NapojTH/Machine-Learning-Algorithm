{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab37193a",
   "metadata": {},
   "source": [
    "## Multiclass Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2371521c",
   "metadata": {},
   "source": [
    "Background: Assume\twe\thave\tN\ttraining\texamples\t{(x1,t1),...,(xN,tN)} where\ttn can\t\n",
    "get\tK discrete\tvalues\t{C1,\t...,\tCK},\ti.e.\ta\tK-class\tclassification\tproblem. We\tuse\ty_n\" to\t\n",
    "represent\tthe\tpredicted\tlabel\tof\tx_n\"\n",
    "\n",
    "Model: To\tsolve\ta\tK-class\tclassification\tproblem,\twe\tcan\tlearn\tK weight\tvectors\t\n",
    "wk,\teach\tof\twhich\tcorresponding\tto\tone\tof\tthe\tclasses.\n",
    "\n",
    "Prediction: In\tthe\tprediction\ttime,\ta\tdata\tpoint\tx\twill\tbe\tclassified\tas\targmaxk wk * x\n",
    "\n",
    "Training\tAlgorithm: We\ttrain\tthe\tmulticlass\tperceptron\tbased\ton\tthe\tfollowing\t\n",
    "algorithm:\n",
    "\n",
    "• Initialise\tthe\tweight\tvectors\trandomly\tw1,..,wK\n",
    "\n",
    "• While\tnot\tconverged\tdo:\n",
    "\n",
    "o For\tn\t=\t1\tto\tN\tdo:\n",
    "- y\t=\targmaxk wk . xn\n",
    "- If\tyn !=\ttn do:\n",
    "\n",
    "• w_yn : = w_yn − η*x_n\"\n",
    "\n",
    "• w_tn : = w_tn + η*x_n\"\n",
    "\n",
    "Based on the background and how the multiclass perceptron work, the code below is how the multiclass perceptron is implemented:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d617e9b",
   "metadata": {},
   "source": [
    "### Import library and pre-process the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bac315",
   "metadata": {},
   "source": [
    "Load Task1D_train.csv and Task1D_test.csv sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b3ff810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n"
     ]
    }
   ],
   "source": [
    "#Import important library for visualisation and reshape the dataset\n",
    "library(ggplot2)\n",
    "library(reshape2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f347732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the file and divide the dataset into label and dataset\n",
    "options(warn=-1)\n",
    "train_data <- read.csv(\"Task1D_train.csv\")\n",
    "test_data <- read.csv(\"Task1D_test.csv\")\n",
    "\n",
    "#Split the data\n",
    "train_dat <- train_data[,-5]\n",
    "train_lab <- train_data[,5]\n",
    "test_dat <- test_data[,-5]\n",
    "test_lab<- test_data[,5]\n",
    "\n",
    "train.len <- nrow(train_data)\n",
    "test.len <- nrow(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0f4e9",
   "metadata": {},
   "source": [
    "### Initialisation (Randomised the weight vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c1efb",
   "metadata": {},
   "source": [
    "In the initialise process, the list of weight vector for each class will be created in order to store each weight vector after prediction. Also, the function for predict the misclassification process is created based on prediction formula explaining in the background information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa917e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(60) #Set the number of ramdomise\n",
    "\n",
    "#Identify how many class in the dataset\n",
    "K<-unique(sort(train_lab)) #All class value\n",
    "Phi <- as.matrix(cbind(1, train_dat)) # add a column of 1 as phi_0\n",
    "eta <- 0.09 # Learning rate\n",
    "epsilon <- 0.01 # Stoping criterion\n",
    "tau.max <- 1000 # Maximum number of iterations\n",
    "T <- train_lab  #Change for convenience\n",
    "\n",
    "#Create a list so that it can initialise multiclass weight vector (In this case we got three classes)\n",
    "weight_list <- list() #make a list so we can get a initial list for different weight vector for each class\n",
    "for (k in 1:length(K)){\n",
    "    weight_list[[k]] <- matrix(,nrow=tau.max, ncol=ncol(Phi)) # Empty Weight vector\n",
    "    weight_list[[k]][1,] <- runif(ncol(Phi)) # Random initial values for weight vector for each class\n",
    "}\n",
    "\n",
    "#Prediction/error calculation function based on argmaxk wk*x \n",
    "error_cal <- function(x,weight,true_label,row){\n",
    "    predictions <- data.frame(matrix(nrow = nrow(x), ncol = length(weight)))\n",
    "    for (j in 1:length(weight)) {\n",
    "        predictions[ , j] = x%*%weight[[j]][row,]\n",
    "    }\n",
    "    T <- as.integer(substr(true_label,2,length(true_label)))  #Change the class to integer so it is convenience for classification\n",
    "    predict_res <- max.col(predictions) #argmaxk wk*x\n",
    "    misclassi_per <- (sum(predict_res != T)/nrow(predictions))*100 #error is sum of the prediction value that not equal to t_n\n",
    "    #divided by all prediction * 100\n",
    "    return(misclassi_per)\n",
    "}\n",
    "\n",
    "\n",
    "error.trace <- matrix(,nrow=tau.max, ncol=1) # Placeholder for errors\n",
    "error.trace[1] <- error_cal(Phi,weight_list,T,1) # record error for initial weights vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a93a5e9",
   "metadata": {},
   "source": [
    "### Multiclass Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9289e",
   "metadata": {},
   "source": [
    "Implement\t the\tmulticlass\tperceptron\tas\texplained\tabove.\tPlease\tprovide enough comments\tfor\tyour\tcode\tin\tyour\tsubmission.\n",
    "\n",
    "In the multiclass perceptron, the sgd algorithm will be used for update weight.  The training algorithm will be using the same approach mentioned in the begining of the task. This is the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ec0504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "2"
      ],
      "text/latex": [
       "2"
      ],
      "text/markdown": [
       "2"
      ],
      "text/plain": [
       "[1] 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>0.764436   </td><td> 0.35277531</td><td>0.6914292  </td><td> 0.69586957</td><td> 0.2826156 </td></tr>\n",
       "\t<tr><td>0.674436   </td><td>-0.21422469</td><td>0.3944292  </td><td> 0.15586957</td><td> 0.0576156 </td></tr>\n",
       "\t<tr><td>0.764436   </td><td> 0.20877531</td><td>0.6824292  </td><td> 0.27286957</td><td> 0.0756156 </td></tr>\n",
       "\t<tr><td>0.764436   </td><td> 0.20877531</td><td>0.6824292  </td><td> 0.27286957</td><td> 0.0756156 </td></tr>\n",
       "\t<tr><td>0.764436   </td><td> 0.20877531</td><td>0.6824292  </td><td> 0.27286957</td><td> 0.0756156 </td></tr>\n",
       "\t<tr><td>0.854436   </td><td> 0.66777531</td><td>0.9974292  </td><td> 0.39886957</td><td> 0.0936156 </td></tr>\n",
       "\t<tr><td>0.764436   </td><td>-0.02522469</td><td>0.7634292  </td><td>-0.22213043</td><td>-0.1133844 </td></tr>\n",
       "\t<tr><td>0.764436   </td><td>-0.02522469</td><td>0.7634292  </td><td>-0.22213043</td><td>-0.1133844 </td></tr>\n",
       "\t<tr><td>0.764436   </td><td>-0.02522469</td><td>0.7634292  </td><td>-0.22213043</td><td>-0.1133844 </td></tr>\n",
       "\t<tr><td>0.764436   </td><td>-0.02522469</td><td>0.7634292  </td><td>-0.22213043</td><td>-0.1133844 </td></tr>\n",
       "\t<tr><td>0.764436   </td><td>-0.02522469</td><td>0.7634292  </td><td>-0.22213043</td><td>-0.1133844 </td></tr>\n",
       "\t<tr><td>0.764436   </td><td>-0.02522469</td><td>0.7634292  </td><td>-0.22213043</td><td>-0.1133844 </td></tr>\n",
       "\t<tr><td>0.764436   </td><td>-0.02522469</td><td>0.7634292  </td><td>-0.22213043</td><td>-0.1133844 </td></tr>\n",
       "\t<tr><td>0.854436   </td><td> 0.44277531</td><td>1.0694292  </td><td>-0.09613043</td><td>-0.0953844 </td></tr>\n",
       "\t<tr><td>0.854436   </td><td> 0.44277531</td><td>1.0694292  </td><td>-0.09613043</td><td>-0.0953844 </td></tr>\n",
       "\t<tr><td>0.854436   </td><td> 0.44277531</td><td>1.0694292  </td><td>-0.09613043</td><td>-0.0953844 </td></tr>\n",
       "\t<tr><td>0.944436   </td><td> 0.83877531</td><td>1.3304292  </td><td> 0.02986957</td><td>-0.0773844 </td></tr>\n",
       "\t<tr><td>0.854436   </td><td> 0.31677531</td><td>1.0874292  </td><td>-0.42913043</td><td>-0.2483844 </td></tr>\n",
       "\t<tr><td>0.944436   </td><td> 0.76677531</td><td>1.3934292  </td><td>-0.28513043</td><td>-0.2123844 </td></tr>\n",
       "\t<tr><td>0.854436   </td><td> 0.30777531</td><td>1.1684292  </td><td>-0.55513043</td><td>-0.3113844 </td></tr>\n",
       "\t<tr><td>0.944436   </td><td> 0.79377531</td><td>1.4744292  </td><td>-0.40213043</td><td>-0.2933844 </td></tr>\n",
       "\t<tr><td>0.854436   </td><td> 0.21777531</td><td>1.2134292  </td><td>-0.78913043</td><td>-0.4103844 </td></tr>\n",
       "\t<tr><td>0.854436   </td><td> 0.21777531</td><td>1.2134292  </td><td>-0.78913043</td><td>-0.4103844 </td></tr>\n",
       "\t<tr><td>0.944436   </td><td> 0.67677531</td><td>1.5554292  </td><td>-0.64513043</td><td>-0.3923844 </td></tr>\n",
       "\t<tr><td>0.944436   </td><td> 0.67677531</td><td>1.5554292  </td><td>-0.64513043</td><td>-0.3923844 </td></tr>\n",
       "\t<tr><td>0.944436   </td><td> 0.67677531</td><td>1.5554292  </td><td>-0.64513043</td><td>-0.3923844 </td></tr>\n",
       "\t<tr><td>0.944436   </td><td> 0.67677531</td><td>1.5554292  </td><td>-0.64513043</td><td>-0.3923844 </td></tr>\n",
       "\t<tr><td>0.944436   </td><td> 0.67677531</td><td>1.5554292  </td><td>-0.64513043</td><td>-0.3923844 </td></tr>\n",
       "\t<tr><td>0.944436   </td><td> 0.67677531</td><td>1.5554292  </td><td>-0.64513043</td><td>-0.3923844 </td></tr>\n",
       "\t<tr><td>0.944436   </td><td> 0.67677531</td><td>1.5554292  </td><td>-0.64513043</td><td>-0.3923844 </td></tr>\n",
       "\t<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "\t<tr><td>1.368225 </td><td>1.778114 </td><td>3.811116 </td><td>-3.275914</td><td>-1.884286</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{lllll}\n",
       "\t 0.764436    &  0.35277531 & 0.6914292   &  0.69586957 &  0.2826156 \\\\\n",
       "\t 0.674436    & -0.21422469 & 0.3944292   &  0.15586957 &  0.0576156 \\\\\n",
       "\t 0.764436    &  0.20877531 & 0.6824292   &  0.27286957 &  0.0756156 \\\\\n",
       "\t 0.764436    &  0.20877531 & 0.6824292   &  0.27286957 &  0.0756156 \\\\\n",
       "\t 0.764436    &  0.20877531 & 0.6824292   &  0.27286957 &  0.0756156 \\\\\n",
       "\t 0.854436    &  0.66777531 & 0.9974292   &  0.39886957 &  0.0936156 \\\\\n",
       "\t 0.764436    & -0.02522469 & 0.7634292   & -0.22213043 & -0.1133844 \\\\\n",
       "\t 0.764436    & -0.02522469 & 0.7634292   & -0.22213043 & -0.1133844 \\\\\n",
       "\t 0.764436    & -0.02522469 & 0.7634292   & -0.22213043 & -0.1133844 \\\\\n",
       "\t 0.764436    & -0.02522469 & 0.7634292   & -0.22213043 & -0.1133844 \\\\\n",
       "\t 0.764436    & -0.02522469 & 0.7634292   & -0.22213043 & -0.1133844 \\\\\n",
       "\t 0.764436    & -0.02522469 & 0.7634292   & -0.22213043 & -0.1133844 \\\\\n",
       "\t 0.764436    & -0.02522469 & 0.7634292   & -0.22213043 & -0.1133844 \\\\\n",
       "\t 0.854436    &  0.44277531 & 1.0694292   & -0.09613043 & -0.0953844 \\\\\n",
       "\t 0.854436    &  0.44277531 & 1.0694292   & -0.09613043 & -0.0953844 \\\\\n",
       "\t 0.854436    &  0.44277531 & 1.0694292   & -0.09613043 & -0.0953844 \\\\\n",
       "\t 0.944436    &  0.83877531 & 1.3304292   &  0.02986957 & -0.0773844 \\\\\n",
       "\t 0.854436    &  0.31677531 & 1.0874292   & -0.42913043 & -0.2483844 \\\\\n",
       "\t 0.944436    &  0.76677531 & 1.3934292   & -0.28513043 & -0.2123844 \\\\\n",
       "\t 0.854436    &  0.30777531 & 1.1684292   & -0.55513043 & -0.3113844 \\\\\n",
       "\t 0.944436    &  0.79377531 & 1.4744292   & -0.40213043 & -0.2933844 \\\\\n",
       "\t 0.854436    &  0.21777531 & 1.2134292   & -0.78913043 & -0.4103844 \\\\\n",
       "\t 0.854436    &  0.21777531 & 1.2134292   & -0.78913043 & -0.4103844 \\\\\n",
       "\t 0.944436    &  0.67677531 & 1.5554292   & -0.64513043 & -0.3923844 \\\\\n",
       "\t 0.944436    &  0.67677531 & 1.5554292   & -0.64513043 & -0.3923844 \\\\\n",
       "\t 0.944436    &  0.67677531 & 1.5554292   & -0.64513043 & -0.3923844 \\\\\n",
       "\t 0.944436    &  0.67677531 & 1.5554292   & -0.64513043 & -0.3923844 \\\\\n",
       "\t 0.944436    &  0.67677531 & 1.5554292   & -0.64513043 & -0.3923844 \\\\\n",
       "\t 0.944436    &  0.67677531 & 1.5554292   & -0.64513043 & -0.3923844 \\\\\n",
       "\t 0.944436    &  0.67677531 & 1.5554292   & -0.64513043 & -0.3923844 \\\\\n",
       "\t ... & ... & ... & ... & ...\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\t 1.368225  & 1.778114  & 3.811116  & -3.275914 & -1.884286\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 0.764436    |  0.35277531 | 0.6914292   |  0.69586957 |  0.2826156  |\n",
       "| 0.674436    | -0.21422469 | 0.3944292   |  0.15586957 |  0.0576156  |\n",
       "| 0.764436    |  0.20877531 | 0.6824292   |  0.27286957 |  0.0756156  |\n",
       "| 0.764436    |  0.20877531 | 0.6824292   |  0.27286957 |  0.0756156  |\n",
       "| 0.764436    |  0.20877531 | 0.6824292   |  0.27286957 |  0.0756156  |\n",
       "| 0.854436    |  0.66777531 | 0.9974292   |  0.39886957 |  0.0936156  |\n",
       "| 0.764436    | -0.02522469 | 0.7634292   | -0.22213043 | -0.1133844  |\n",
       "| 0.764436    | -0.02522469 | 0.7634292   | -0.22213043 | -0.1133844  |\n",
       "| 0.764436    | -0.02522469 | 0.7634292   | -0.22213043 | -0.1133844  |\n",
       "| 0.764436    | -0.02522469 | 0.7634292   | -0.22213043 | -0.1133844  |\n",
       "| 0.764436    | -0.02522469 | 0.7634292   | -0.22213043 | -0.1133844  |\n",
       "| 0.764436    | -0.02522469 | 0.7634292   | -0.22213043 | -0.1133844  |\n",
       "| 0.764436    | -0.02522469 | 0.7634292   | -0.22213043 | -0.1133844  |\n",
       "| 0.854436    |  0.44277531 | 1.0694292   | -0.09613043 | -0.0953844  |\n",
       "| 0.854436    |  0.44277531 | 1.0694292   | -0.09613043 | -0.0953844  |\n",
       "| 0.854436    |  0.44277531 | 1.0694292   | -0.09613043 | -0.0953844  |\n",
       "| 0.944436    |  0.83877531 | 1.3304292   |  0.02986957 | -0.0773844  |\n",
       "| 0.854436    |  0.31677531 | 1.0874292   | -0.42913043 | -0.2483844  |\n",
       "| 0.944436    |  0.76677531 | 1.3934292   | -0.28513043 | -0.2123844  |\n",
       "| 0.854436    |  0.30777531 | 1.1684292   | -0.55513043 | -0.3113844  |\n",
       "| 0.944436    |  0.79377531 | 1.4744292   | -0.40213043 | -0.2933844  |\n",
       "| 0.854436    |  0.21777531 | 1.2134292   | -0.78913043 | -0.4103844  |\n",
       "| 0.854436    |  0.21777531 | 1.2134292   | -0.78913043 | -0.4103844  |\n",
       "| 0.944436    |  0.67677531 | 1.5554292   | -0.64513043 | -0.3923844  |\n",
       "| 0.944436    |  0.67677531 | 1.5554292   | -0.64513043 | -0.3923844  |\n",
       "| 0.944436    |  0.67677531 | 1.5554292   | -0.64513043 | -0.3923844  |\n",
       "| 0.944436    |  0.67677531 | 1.5554292   | -0.64513043 | -0.3923844  |\n",
       "| 0.944436    |  0.67677531 | 1.5554292   | -0.64513043 | -0.3923844  |\n",
       "| 0.944436    |  0.67677531 | 1.5554292   | -0.64513043 | -0.3923844  |\n",
       "| 0.944436    |  0.67677531 | 1.5554292   | -0.64513043 | -0.3923844  |\n",
       "| ... | ... | ... | ... | ... |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "| 1.368225  | 1.778114  | 3.811116  | -3.275914 | -1.884286 |\n",
       "\n"
      ],
      "text/plain": [
       "      [,1]     [,2]        [,3]      [,4]        [,5]      \n",
       " [1,] 0.764436  0.35277531 0.6914292  0.69586957  0.2826156\n",
       " [2,] 0.674436 -0.21422469 0.3944292  0.15586957  0.0576156\n",
       " [3,] 0.764436  0.20877531 0.6824292  0.27286957  0.0756156\n",
       " [4,] 0.764436  0.20877531 0.6824292  0.27286957  0.0756156\n",
       " [5,] 0.764436  0.20877531 0.6824292  0.27286957  0.0756156\n",
       " [6,] 0.854436  0.66777531 0.9974292  0.39886957  0.0936156\n",
       " [7,] 0.764436 -0.02522469 0.7634292 -0.22213043 -0.1133844\n",
       " [8,] 0.764436 -0.02522469 0.7634292 -0.22213043 -0.1133844\n",
       " [9,] 0.764436 -0.02522469 0.7634292 -0.22213043 -0.1133844\n",
       "[10,] 0.764436 -0.02522469 0.7634292 -0.22213043 -0.1133844\n",
       "[11,] 0.764436 -0.02522469 0.7634292 -0.22213043 -0.1133844\n",
       "[12,] 0.764436 -0.02522469 0.7634292 -0.22213043 -0.1133844\n",
       "[13,] 0.764436 -0.02522469 0.7634292 -0.22213043 -0.1133844\n",
       "[14,] 0.854436  0.44277531 1.0694292 -0.09613043 -0.0953844\n",
       "[15,] 0.854436  0.44277531 1.0694292 -0.09613043 -0.0953844\n",
       "[16,] 0.854436  0.44277531 1.0694292 -0.09613043 -0.0953844\n",
       "[17,] 0.944436  0.83877531 1.3304292  0.02986957 -0.0773844\n",
       "[18,] 0.854436  0.31677531 1.0874292 -0.42913043 -0.2483844\n",
       "[19,] 0.944436  0.76677531 1.3934292 -0.28513043 -0.2123844\n",
       "[20,] 0.854436  0.30777531 1.1684292 -0.55513043 -0.3113844\n",
       "[21,] 0.944436  0.79377531 1.4744292 -0.40213043 -0.2933844\n",
       "[22,] 0.854436  0.21777531 1.2134292 -0.78913043 -0.4103844\n",
       "[23,] 0.854436  0.21777531 1.2134292 -0.78913043 -0.4103844\n",
       "[24,] 0.944436  0.67677531 1.5554292 -0.64513043 -0.3923844\n",
       "[25,] 0.944436  0.67677531 1.5554292 -0.64513043 -0.3923844\n",
       "[26,] 0.944436  0.67677531 1.5554292 -0.64513043 -0.3923844\n",
       "[27,] 0.944436  0.67677531 1.5554292 -0.64513043 -0.3923844\n",
       "[28,] 0.944436  0.67677531 1.5554292 -0.64513043 -0.3923844\n",
       "[29,] 0.944436  0.67677531 1.5554292 -0.64513043 -0.3923844\n",
       "[30,] 0.944436  0.67677531 1.5554292 -0.64513043 -0.3923844\n",
       "[31,] ...      ...         ...       ...         ...       \n",
       "[32,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[33,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[34,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[35,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[36,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[37,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[38,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[39,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[40,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[41,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[42,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[43,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[44,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[45,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[46,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[47,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[48,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[49,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[50,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[51,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[52,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[53,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[54,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[55,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[56,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[57,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[58,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[59,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[60,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 \n",
       "[61,] 1.368225 1.778114    3.811116  -3.275914   -1.884286 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(60)\n",
    "tau <- 1 # iteration counter \n",
    "terminate <- FALSE # termination status\n",
    "\n",
    "#SGD for perceptron derived from Activity 3.1 from tutorial and some modification\n",
    "while(!terminate){\n",
    "    # Shuffle the data\n",
    "    train_index <- sample(1:train.len, replace = FALSE)\n",
    "    Phi <- Phi[train_index, ]\n",
    "    T <- T[train_index]\n",
    "    \n",
    "    #For each number of iteration for each training point\n",
    "    for (i in 1:train_index){\n",
    "        #If the the number of iteration met the criteria, then it stop\n",
    "        if (tau == tau.max) {break}\n",
    "        \n",
    "        predict_val <- c()\n",
    "        \n",
    "        #For each K-class, Predict the class whose weight vector produces the highest score\n",
    "        for (k in 1:length(K)){\n",
    "            #Calculate each prediction of the model\n",
    "            predict_val[k] <- weight_list[[k]][tau,]%*%Phi[i,] #weight vector * x\n",
    "        }\n",
    "        \n",
    "        #Predict value yn(argmaxk wk*x) \n",
    "        pred_value <- which.max(predict_val) #Identify which k-class weight vector produce highest :yn\n",
    "        real_value <- which(K==T[i])#real value tn\n",
    "        \n",
    "        #If the predict value is not equal to real value\n",
    "        if (pred_value!=real_value){\n",
    "            #Increase iteration by 1\n",
    "            tau = tau + 1\n",
    "            #Update the weight\n",
    "            for (k in 1:length(K)) {\n",
    "                weight_list[[k]][tau,] <- weight_list[[k]][tau-1,] # update all W's with W-1\n",
    "                }\n",
    "            #Update the incorrect prediction based on these formula: w_pred = w_pred - eta*x and w_real = w_real + eta*x\n",
    "            weight_list[[real_value]][tau,] <- weight_list[[real_value]][tau-1,] + eta*Phi[i,] #increase score of the right answer\n",
    "            weight_list[[pred_value]][tau,] <- weight_list[[pred_value]][tau-1,]-eta*Phi[i,]#reduce score of wrong answer\n",
    "            \n",
    "            #Calculate the error\n",
    "            error.trace[tau,] <- error_cal(Phi,weight_list,T,tau)\n",
    "        }\n",
    "    }\n",
    "    #Decrease eta\n",
    "    eta <- eta*0.99\n",
    "    # recalculate termination conditions\n",
    "    terminate <- (tau >= tau.max | (error.trace[tau, ] <= epsilon))\n",
    "}\n",
    "#cut the empty part of the matrix (when the loop stops before tau == tau.max)\n",
    "weight<-list() \n",
    "for (i in 1:length(K)){\n",
    "    weight[[i]] <- weight_list[[i]][1:tau,]\n",
    "    }\n",
    "\n",
    "#argmax k so we use this in the error prediction \n",
    "pred_value\n",
    "weight[[1]] #all the weight vector in the argmax k that updated over the iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ccedf",
   "metadata": {},
   "source": [
    "### Plot the error vs the number of mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded74e6",
   "metadata": {},
   "source": [
    "Set\t the\tlearning\trate\tη\t to\t.09,\tand\ttrain\t the\tmulticlass\tperceptron\ton\tthe provided training\tdata.After\tprocessing\tevery\t5 training\tdata\tpoints\t(also known\tas\ta mini-batch),\tevaluate\t the\t error\t of\t the\t current\tmodel\t on\t the  test\t data.\t Plot\t the\t error\t of the\t test\t data\t vs\t the\t number\t of\tmini-batches,and include\tit\tin\tyour\tJupyter\tNotebook\tfile\tfor\tQuestion\t6.\t\n",
    "\n",
    "Now, the error produce by the model from training set will be used for evaluate error of testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6da972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABwgAAALQCAMAAACzGNRRAAAANlBMVEUAAABNTU1oaGh8fHyM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\nIgQAAGkQIQAASIMIAQBAGkQIAADS/A9UhKQs2rFE7AAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Making phi-0 for test set\n",
    "Phi_testset <- as.matrix(cbind(1, test_dat))\n",
    "T <- test_lab # rename for simplicity\n",
    "test_error <- data.frame() #dataframe for adding error rate \n",
    "\n",
    "\n",
    "#Iteration of all the test point by 5 data points (Evaluate the error based on the training of multi perceptron)\n",
    "t=1 #counter \n",
    "for(j in seq(from=1, to =nrow(weight[[1]]), by = 5)){ #using weight[[1]] based on the argmax k \n",
    "    test_error[t,\"eta0.09\"] = error_cal(Phi_testset,weight,T,j)\n",
    "    t=t+1\n",
    "}\n",
    "test_error[,\"iteration\"] <- as.numeric(rownames(test_error))\n",
    "# reshaping for ggplot\n",
    "error_test.m <- melt(test_error, id.vars = \"iteration\",value.name = \"Error\", variable.name = \"Eta\")\n",
    "# adjusting plot size\n",
    "options(repr.plot.width = 15, repr.plot.height = 6)\n",
    "# plotting\n",
    "ggplot(data = error_test.m, mapping = aes(x =iteration, y = Error, color = Eta)) + geom_line() +\n",
    "    scale_color_discrete(guide = guide_legend(title = 'Test Errors by Learning Rate')) + theme_minimal()+ggtitle(\"Error vs iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20ea4d8",
   "metadata": {},
   "source": [
    "Based on the plot of error vs iteration, it seems that the error rate is decreased once the iteration of batch is increase for eta = 0.09. Also, the when the iteration is about 50, the error (convergence) is almost zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a874f97d",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81843bf3",
   "metadata": {},
   "source": [
    "All of the code and the algorithm idea is derieved from background information and from the following:\n",
    "\n",
    "- Chen, B. (2022). $\\textit{Week 5.: Linear Models for Classification}$ \\[PowerPoint slides]. https://lms.monash.edu/mod/resource/view.php?id=9894993\n",
    "- Jupyter Notebooks:FIT5201 Machine Learning, (nd.). $\\textit{Activity 3.1 Perceptron}$. \n",
    "https://lms.monash.edu/mod/folder/view.php?id=10133948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae428b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
